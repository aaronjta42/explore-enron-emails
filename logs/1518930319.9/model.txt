Model Structure:
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 148)               44548     
_________________________________________________________________
activation_1 (Activation)    (None, 148)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 296)               44104     
_________________________________________________________________
activation_2 (Activation)    (None, 296)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 148)               43956     
_________________________________________________________________
activation_3 (Activation)    (None, 148)               0         
=================================================================
Total params: 132,608
Trainable params: 132,608
Non-trainable params: 0
_________________________________________________________________


========================================================


Model Configuration:

Layer 0:

{"kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "scale": 1.0, "seed": null, "mode": "fan_avg"}}, "name": "dense_1", "kernel_constraint": {"class_name": "MaxNorm", "config": {"max_value": Infinity, "axis": 0}}, "bias_regularizer": null, "bias_constraint": null, "dtype": "float32", "activation": "linear", "trainable": true, "kernel_regularizer": {"class_name": "L1L2", "config": {"l2": 0.0, "l1": 0.0}}, "bias_initializer": {"class_name": "Zeros", "config": {}}, "units": 148, "batch_input_shape": [null, 300], "use_bias": true, "activity_regularizer": null}

Layer 1:

{"activation": "relu", "trainable": true, "name": "activation_1"}

Layer 2:

{"kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "scale": 1.0, "seed": null, "mode": "fan_avg"}}, "name": "dense_2", "kernel_constraint": {"class_name": "MaxNorm", "config": {"max_value": Infinity, "axis": 0}}, "bias_regularizer": null, "bias_constraint": null, "activation": "linear", "trainable": true, "kernel_regularizer": {"class_name": "L1L2", "config": {"l2": 0.0, "l1": 0.0}}, "bias_initializer": {"class_name": "Zeros", "config": {}}, "units": 296, "use_bias": true, "activity_regularizer": null}

Layer 3:

{"activation": "relu", "trainable": true, "name": "activation_2"}

Layer 4:

{"kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "scale": 1.0, "seed": null, "mode": "fan_avg"}}, "name": "dense_3", "kernel_constraint": {"class_name": "MaxNorm", "config": {"max_value": Infinity, "axis": 0}}, "bias_regularizer": null, "bias_constraint": null, "activation": "linear", "trainable": true, "kernel_regularizer": {"class_name": "L1L2", "config": {"l2": 0.0, "l1": 0.0}}, "bias_initializer": {"class_name": "Zeros", "config": {}}, "units": 148, "use_bias": true, "activity_regularizer": null}

Layer 5:

{"activation": "softmax", "trainable": true, "name": "activation_3"}

